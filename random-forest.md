# 随机森林算法

## 简单介绍
新兴起的、高度灵活的一种机器学习算法。是一个包含多个决策树的分类器。属于分类算法的一种。
![Alt text](https://img-blog.csdn.net/20160418213046099 "机器学习大分类")


## 详细介绍
 - 决策树：树结构，根据资源成本和风险作为判断依据，引导决策结果的一种方式
 - 一群决策树：即随机森林  
 
每棵树都是一个分类器（决策的工具），N颗树就有N个分类结果。随机森林就是集成所有决策树的投票结果，将投票次数最多(众数)的类别指定为最终输出。引导聚集的思想(bagging)。

## 特点
 - 数据越多，分类器越准确，对大量数据输入友好。
 - 决策树森林，不会因为某个树的误差而影响最终输出
 - 决策数据可丢失，并且，如果有很大一部分的数据丢失，仍可以维持准确度。
 - 对于不平衡的分类数据集来说，它可以平衡误差。
 - 训练学习速度快  
 
## 如何创建一个随机森林 // TODO
#### 第一步，先创建一棵树:
 - 用N来表示训练用例（样本）的个数，M表示特征数目。
 - 输入特征数目m，用于确定决策树上一个节点的决策结果；其中m应远小于M。
 - 从N个训练用例（样本）中以有放回抽样的方式，取样N次，形成一个训练集（即bootstrap取样），并用未抽到的用例（样本）作预测，评估其误差。
 - 对于每一个节点，随机选择m个特征，决策树上每个节点的决定都是基于这些特征确定的。根据这m个特征，计算其最佳的分裂方式。
 - 每棵树都会完整成长而不会剪枝（Pruning，这有可能在建完一棵正常树状分类器后会被采用）。  
 
#### 第二步，再创建N颗树
 - 循环第一步